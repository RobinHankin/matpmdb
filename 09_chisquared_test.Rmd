# Pearson's chi-square test {#chisquare}

```{block2, type='wikipedia'}
<https://en.wikipedia.org/wiki/Chi-squared_test>
```

```{block2, type='youtube'}
<https://www.youtube.com/watch?v=OkatmisQs8E&index=14&list=PL018X5Hlr4RkgE65Pg93TFY-32KCVpW84>
```

```{block2, type='youtube'}
<https://www.youtube.com/watch?v=al-2-RRzMek&index=15&list=PL018X5Hlr4RkgE65Pg93TFY-32KCVpW84>
```

Suppose I poll a class of 40 students and ask each one for their
favourite colour out of red, green, blue, and yellow.  My observations are:

```{r}
o <- c(red=8, green=10, blue=17, yellow= 5)
o
```

("o" for "observation").  Thus 8 students chose red, 10 chose green,
etc.  Is there any evidence that there is a systematic bias towards
some colours?  Well, the first step is to formulate a null hypothesis
$H_0\colon p_\mathrm{red} = p_\mathrm{green} = p_\mathrm{blue} =
p_\mathrm{yellow}=\frac{1}{4}$, where $p_\mathrm{colour}$ is the
probability of a student choosing that colour.  If the null is true,
we would expect to observe 10 students choosing each colour:

```{r}
e <- c(red=10, green=10, blue=10, yellow= 10)
e
```

("e" for "expectation").  We need to quantify the difference between
observation and expectation^[Remember: the whole of science reduces to
a comparison between observation and expectation] as a single number.
What we want is to calculate a number which is *small* if the
observations are close to expectations, and *large* if the
observations are far away from expectations.  I usually call this
number $B$, for "badness of fit".

It turns out that one particular definition of $B$ has nice properties
that we can work with:

\begin{equation}
B=\sum_i\frac{\left(e_i-o_i\right)^2}{e_i}
\end{equation}

where $o_i$ is the $i^\mathrm{th}$ observation and $e_i$ is the
$i^\mathrm{th}$ expectation.  This is easy to calculate:

```{r}
(8-10)^2/10 + (10-10)^2/10 + (17-10)^2/10 + (5-10)^2/10
```

but of course there is an easier way:

```{r}
B <- sum((o-e)^2/e)
B
```

See how the definition of $B$ ensures that if the observations were
exactly equal to the expectations, $B$ would be zero.  We can also see
that $B$ cannot be negative (because each term is a squared number
divided by a positive number), and also that $B$ is large if the
expectations are very different from the observations.

Recall the definition of p-value: "the probability, if the null is
true, of obtaining the observation or an observation more extreme".
Here, our observation is the value of $B=7.8$.  Our job is to figure
out what the distribution of $B$ is, given that the null is true.  If
we assume that the expected number of observations is not too small
($>5$ is the usual heuristic), it turns out that the null distribution
of $B$ has a pleasing mathematical form, called the *chi-squared
distribution*.

## The chi squared distribution

```{block2, type='wikipedia'}
<https://en.wikipedia.org/wiki/Chi-squared_distribution>
```

The chi squared distribution, sometimes denoted $\chi^2$, is useful
for many reasons but here we need it because it tells us about the
distribution of $B$ if the null is true.

The $\chi^2$ distribution does not have parameters in the same way
that the Gaussian distribution has $\mu$ and $\sigma$.  But it does
have "degrees of freedom" which is an nonnegative integer usually
written as a subscript: $\chi^2_n$.  The degrees of freedom varies
from problem to problem but is easy to calculate.  The R idiom below
shows some chi-square distributions.

```{r}
x <- seq(from=0,to=20,len=1000)  # values for horizontal axis
plot(1:10,1:10,type='n',xlim=c(0,10),ylim=c(0,0.5),xlab='x',ylab='PDF',
  main='chi-squared distribution')  # setup plot axes and title
for(i in 1:10){  # loop over i
    points(x,dchisq(x,df=i),type='l',lwd=2,col=rainbow(10)[i]) #plot one curve
}
legend("topright",lwd=2,col=rainbow(10),legend=paste("df= " ,1:10,sep=""))
```

Study the above R idiom carefully, and note how the distributions move
to the right with increasing degrees of freedom.

## The chi squared distribution and Pearson's chi-squared test

It turns out that if the null is true then $B$, the badness of fit
measure defined above, follows a chi-squared distribution.  The number
of degrees of freedom is given by the number of cells (in the students
choosing colours example above, this would be 4) minus one, which
would be $4-1=3$ degrees of freedom.  The reason you subtract one from
the number of cells is that knowing three cells is enough to calculate
the fourth, because we know how many students are in the class.


```{r}
x <- seq(from=0,to=12,len=1000)  # set up x-axis
plot(x,dchisq(x,df=3),type='l',lwd=2,main="Pearson's chi square test")  # setup axes
B <- 7.8  # value of Badness from above
abline(v=B) # draw vertical line
jj <- seq(from=B,to=12,len=100)  # temporary variable
polygon(c(jj,rev(jj)),c(jj*0,dchisq(rev(jj),df=3)),col='gray',border=NA)  #shade pvalue
text(7.8,0.1,"observed B",pos=2)  
```

In the above figure, the p-value is shown in gray: it is the
probability, if the null is true, of obtaining the observation or an
observation more extreme.  In this case, the observation is $B=7.8$
and its null distribution is $\chi^2_3$.  Calculating the pvalue is straightforward:

```{r}
pchisq(7.8,df=3,lower.tail=FALSE)
```

just short of the 5\% critical value, so we fail to reject the null.

## Numerical verification

Above, I stated that $B$ has a chi-square distribution with 3 degrees
of freedom.  Here I demonstrate that this is true.  We can generate
synthetic observations using the `sample()` function:


```{r}
sample(1:4,40,replace=T)  # census of the class
tabulate(sample(1:4,40,replace=TRUE))  # how many students prefer each colour

```

(we identify red=1, green=2, etc).  We can calculate B for such
synthetic data:

```{r}
o <- tabulate(sample(1:4,40,replace=TRUE))  # synthetic observations
e <- 10                                  # expectation under the null
B <- sum((o-e)^2/e)                      # Badness-of-fit
B
```

And I assert that $B$ is drawn from $\chi^2_3$.  This is straightforward to verify in R:

```{r}
f <- function(...){  # define single-use function f()
   o <- tabulate(sample(1:4,40,replace=TRUE))  # synthetic observations
   e <- 10  # expectations
   return(sum((o-e)^2/e))  # return badness B
}
n <- 1e5   # number of synthetic observations to take 
hist(replicate(n,f()),col='red',nclass=30)  # plot the histogram of synth.obs
x <- seq(from=0,to=10,len=100)   # x-values for theoretical distribution
points(x,n*dchisq(x,df=3),type='l',lwd=6)  # plot chi-squared, df=3 points
```


## Another example

Here I will give an example along the same lines as above but with a
different null.  Suppose we give 100 students a coin and tell them to
toss it 4 times.  We can then ask each student to write down the
number of heads they get.   First, our observations:

```{r}
o <-c("0"=5, "1"=27, "2" = 47, "3"=15, "4"=6)  # this is a named vector
o
```

The expectations are a little harder.  Our null is that the number of
heads is $\operatorname{Bin}(4,1/2)$, and we have 100 observations, so our expectations are given by

```{r}
e <- 100*dbinom(0:4,4,1/2)
e
```

(note that the expected values are not necessarily integer-valued).
Now the badness of fit:

```{r}
B <- sum((o-e)^2/e)
B
```

And the p-value is


```{r}
pchisq(B,df=4,lower.tail=FALSE)
```

So the p-value exceeds 5\% and the result is not significant.


## Pearson chi-square test with estimated parameters

Sometimes we might have a null hypothesis that includes an unknown
parameter.  Suppose we are looking at Robin's photograph album with 23
photos in it.  Robin has 3 children and we know he likes taking photos
of his kids.  We count how many photos have 0,1,2,3 children in them
and observe the following dataset:


```{r}
( o <- c("0"=7,"1"=3,"2"=4,"3"=9))
```

We might hypotheize that the number of children in each photo is
binomial $\operatorname{Bin}(3,p)$ but we do not know the value of
$p$.  To estimate $p$, we figure out how many photos of the three
children there are and divide by how many there could possibly be:

```{r}
phat <- (0*7 + 1*3 + 2*4 + 3*9)/(3*(7+3+4+9))
```

or

```{r}
(phat <- sum((0:3)*o)/(3*sum(o)))
```

Armed with this, we can calculate expectations:

```{r}
(e <- 23*dbinom(0:3,3,phat))
```

and the p-value is easy to calculate

```{r}
pval <- pchisq(sum((o-e)^2/e),df=2,lower.tail=FALSE)
pval
```

showing that we may reject the null: we have strong evidence that the
number of children in a photo is not binomially distributed.

###  Note on number of degrees of freedom

In the photo example above, note that the number of degrees of freedom
is 2, not 3 as might be expected.  This is because we have to subtract
an additional degree of freedom to compensate for the fact that
`phat` was calculated from the data.  Because `phat` is a
single parameter, this corresponds to a single degree of freedom which
has to be subtracted from the `pchisq()` calculation.  Some more
sophisticated tests involve distributions with multiple parameters;
one has to subtract one degree of freedom per parameter estimated from
the data.

## Chi-square test and the Poisson distribution

The chi-square test works well with the Poisson, but we have to be
careful with small tail probabilities.  Here we will consider the
number of goals scored by Manchester City Football club in the
2016-2017 season, data courtesy of wikipedia.

```{block2, type='wikipedia'}
<https://en.wikipedia.org/wiki/2016%E2%80%9317_Manchester_City_F.C._season>
```

The dataset is

```{r}
manC <- c(2,4,3,1,4,3,0,1,1,4,1,2,2,1,2,2,2,3,0,2,0,2,4,2,2,0,1,2,1,3,3,5,3,0,5,2,2,0)
```

Thus in the first game, they scored 2 goals, in the second 4, and so
on up to game number 38 which scored zero goals.  We wish to test
whether the number of goals is Poisson.  This would be a reasonable
supposition on the grounds that there are a large number of minutes in
a football match, each one of which would have a small probability of
scoring a goal.  The first step would be to tabulate the data:


```{r}
o <- table(manC)    #'o' for 'observations'
o
```

Thus, we have six matches with zero goals, 7 matches with 1 goal, 13
matches with 2 goals, and so on.  Now we need to calculate an
expectation to compare these observations against.  The mean number of
goals is:

```{r}
mean(manC)
```

that is, a little over two goals per match.  We can model the number
of goals scored in each match as a Poisson distribution with
$\lambda=2.026$.  We will classify games as having 0,1,2,3,4, or $\geq
5$ goals (this device prevents expected numbers being too small for
the Chi square test to operate).  The probabilities of 0-4 goals is

```{r}
dpois(0:4, lambda=mean(manC))
```

Then the probability of scoring 5 goals will be the remaining probability, viz

```{r}
1-sum(dpois(0:4, lambda=mean(manC)))
```

So the probabilities of $0,1,2,3,4,\geq5$ goals is

```{r}
probs <- c(dpois(0:4, lambda=mean(manC)), ppois(4,lambda=mean(manC),lower.tail=FALSE))
probs
```

check:

```{r}
sum(probs)
```

Then the expected number of games with $0,1,2,3,4,\geq 5$ goals is:

```{r}
e <- length(manC)*probs
e
```

Thus we expect to see about 5 games with zero goals, 10 with one goal,
etc.  The chi-square test is straightforward:

```{r}
B <- sum((o-e)^2/e)
pchisq(B,df=4,lower.tail=FALSE)
```

and we can see that we fail to reject the null: the observations are
consistent with a Poisson distribution.  For the degrees of freeedom,
note that we subtract (from 6, the number of entries in the table) one
because we know the total number of games played, and another one
because we needed to use the data to estimate $\lambda$ which was used
to generate the expectations.  Thus we use 6-1-1=4.
