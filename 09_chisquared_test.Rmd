# Pearson's chi-square test {#chisquare}

```{block2, type='wikipedia'}
<https://en.wikipedia.org/wiki/Chi-squared_test>
```

Suppose I poll a class of 40 students and ask each one for their
favourite colour out of red, green, blue, and yellow.  My observations are:

```{r}
o <- c(red=8, green=10, blue=17, yellow= 5)
o
```

("o" for "observation").  Thus 8 students chose red, 10 chose green,
etc.  Is there any evidence that there is a systematic bias towards
some colours?  Well, the first step is to formulate a null hypothesis
$H_0\colon p_\mathrm{red} = p_\mathrm{green} = p_\mathrm{blue} =
p_\mathrm{yellow}=\frac{1}{4}$, where $p_\mathrm{colour}$ is the
probability of a student choosing that colour.  If the null is true,
we would expect to observe 10 students choosing each colour:

```{r}
e <- c(red=10, green=10, blue=10, yellow= 10)
e
```

("e" for "expectation").  We need to quantify the difference between
observation and expectation^[Remember: the whole of science reduces to
a comparison between observation and expectation] as a single number.
What we want is to calculate a number which is *small* if the
observations are close to expectations, and *large* if the
observations are far away from expectations.  I usually call this
number $B$, for "badness of fit".

It turns out that one particular definition of $B$ has nice properties
that we can work with:

\begin{equation}
B=\sum_i\frac{\left(e_i-o_i\right)^2}{e_i}
\end{equation}

where $o_i$ is the $i^\mathrm{th}$ observation and $e_i$ is the
$i^\mathrm{th}$ expectation.  This is easy to calculate:

```{r}
(8-10)^2/10 + (10-10)^2/10 + (17-10)^2/10 + (5-10)^2/10
```

but of course there is an easier way:

```{r}
B <- sum((o-e)^2/e)
B
```

See how the definition of $B$ ensures that if the observations were
exactly equal to the expectations, $B$ would be zero.  We can also see
that $B$ cannot be negative (because each term is a squared number
divided by a positive number), and also that $B$ is large if the
expectations are very different from the observations.

Recall the definition of p-value: "the probability, if the null is
true, of obtaining the observation or an observation more extreme".
Here, our observation is the value of $B=7.8$.  Our job is to figure
out what the distribution of $B$ is, given that the null is true.  It
turns out that the null distribution of $B$ has a pleasing
mathematical form, called the *chi-squared distribution*.

## The chi squared distribution

```{block2, type='wikipedia'}
<https://en.wikipedia.org/wiki/Chi-squared_distribution>
```

The chi squared distribution, sometimes denoted $\chi^2$, is useful
for many reasons but here we need it because it tells us about the
distribution of $B$ if the null is true.

The $\chi^2$ distribution does not have parameters in the same way
that the Gaussian distribution has $\mu$ and $\sigma$.  But it does
have "degrees of freedom" which is an nonnegative integer usually
written as a subscript: $\chi^2_n$.  The degrees of freedom varies
from problem to problem but is easy to calculate.  The R idiom below
shows some chi-square distributions.

```{r}
x <- seq(from=0,to=20,len=1000)
plot(1:10,1:10,type='n',xlim=c(0,10),ylim=c(0,0.5),xlab='x',ylab='PDF',main='chi-squared distribution')
for(i in 1:10){
    points(x,dchisq(x,df=i),type='l',lwd=2,col=rainbow(10)[i])
}
legend("topright",lwd=2,col=rainbow(10),legend=paste("df= " ,1:10,sep=""))
```

Study the above R idiom carefully, and note how the distributions move
to the right with increasing degrees of freedom.

## The chi squared distribution and Pearson's chi-squared test

It turns out that if the null is true then $B$, the badness of fit
measure defined above, follows a chi-squared distribution.  The number
of degrees of freedom is given by the number of cells (in the students
choosing colours example above, this would be 4) minus one, which
would be $4-1=3$ degrees of freedom.  The reason you subtract one from
the number of cells is that knowing three cells is enough to calculate
the fourth, because we know how many students are in the class.


```{r}
x <- seq(from=0,to=12,len=1000)  # set up x-axis
plot(x,dchisq(x,df=3),type='l',lwd=2,main="Pearson's chi square test")  # setup axes
B <- 7.8  # value of Badness from above
abline(v=B) # draw vertical line
jj <- seq(from=B,to=12,len=100)  # temporary variable
polygon(c(jj,rev(jj)),c(jj*0,dchisq(rev(jj),df=3)),col='gray',border=NA)  #shade pvalue
text(7.8,0.1,"observed B",pos=2)  
```

In the above figure, the p-value is shown in gray: it is the
probability, if the null is true, of obtaining the observation or an
observation more extreme.  In this case, the observation is $B=7.8$
and its null distribution is $\chi^2_3$.  Calculating the pvalue is straightforward:

```{r}
pchisq(7.8,df=3,lower.tail=FALSE)
```

just short of the 5\% critical value, so we fail to reject the null.

## Numerical verification

Above, I stated that $B$ has a chi-square distribution with 3 degrees
of freedom.  Here I demonstrate that this is true.  We can generate
synthetic observations using the ```sample()``` function:


```{r}
sample(1:4,40,replace=T)  # census of the class
tabulate(sample(1:4,40,replace=TRUE))  # how many students prefer each colour

```

(we identify red=1, green=2, etc).  We can calculate B for such
synthetic data:

```{r}
o <- tabulate(sample(1:4,40,replace=TRUE))  # synthetic obseravations
e <- 10                                  # expectation under the null
B <- sum((o-e)^2/e)                      # Badness-of-fit
B
```

And I assert that $B$ is drawn from $\chi^2_3$.  This is straightforward to verify in R:

```{r}
f <- function(...){
   o <- tabulate(sample(1:4,40,replace=TRUE))  # synthetic obseravations
   e <- 10
   return(sum((o-e)^2/e))
}
hist(replicate(1000,f()),col='red')
```


## Another example

Here I will give an example along the same lines as above but with a
different null.  Suppose we give 40 students a coin and tell them to
toss it 5 times.  We can then ask each student to write down the
number of heads they get. 
