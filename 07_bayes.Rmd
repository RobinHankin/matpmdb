# Bayes's theorem {#bayes}

```{block2, type='wikipedia'}
<https://en.wikipedia.org/wiki/Bayes%27_theorem>
```

IMPORTANT:

* "Bayes theorem": incorrect; no possession marker
* "Bayes' theorem": incorrect (American usage)
* "Bayes's theorem": correct British English

Do not get this wrong.


```{block2, type='youtube'}
<https://www.youtube.com/watch?v=-q6KG_ZurcU&index=8&list=PL018X5Hlr4RkgE65Pg93TFY-32KCVpW84>
```

```{block2, type='youtube'}
<https://www.youtube.com/watch?v=AK3h_4LPl_Q&list=PL018X5Hlr4RkgE65Pg93TFY-32KCVpW84&index=9>
```

```{block2, type='youtube'}
<https://www.youtube.com/watch?v=ib7juP-ZBNk&list=PL018X5Hlr4RkgE65Pg93TFY-32KCVpW84&index=10>
```

```{block2, type='youtube'}
<https://www.youtube.com/watch?v=CxORosyXS5A&index=11&list=PL018X5Hlr4RkgE65Pg93TFY-32KCVpW84>
```

In this chapter I will discuss a totally different philosophy for
statistics, called the *Bayesian* approach.  This is very different
from the hypothesis testing material presented above; it is arguably
more consistent and coherent than the frequentist approach (null
hypotheses and type I/type II errors; confidence intervals).

One feature of modern statistics is that statisticians are often
expected to declare themselves as either "Bayesians" or
"frequentists", depending on whether or not they buy into the Bayesian
philosophy.  My estimate would be that 30\% of statisticians are
Bayesians.

```{r venn, fig.cap='Two sets, $A$ and $B$ on a Venn diagram together with red areas marking set union ($A\\cup B$), set intersection ($A\\cap B$), complement of A ($\\overline{A}$) and symmetric difference ($A\\Delta B$)', echo=FALSE, message=FALSE, warning=FALSE}
knitr::include_graphics("intersection_union.pdf")
```

In figure \@ref(fig:venn) we see four set diagrams. Top left, set
union $A\cup B$; top right; set intersection $A\cap B$; lower left,
the complement of $A$, $\overline{A}$, that is, everything not in $A$;
lower right, symmetric difference $A\triangle B$.  Think of A and B
being events with specific probabilities, a good example might be
A="it is raining" and B="Robin cycles to work".  Note that Robin very
rarely cycles to work if it is raining (he gets the bus instead).  We
can define probabilities $\prob{A}$ and $\prob{B}$.  We can also
define $\prob{A|B}$ as the probability of A given B.  This would be
the probability of it raining, given that Robin cycles to work.
Observe that this differs from $\prob{B|A}$, the probability of B
given A, which would be the probability of cycling to work given that
it is raining.

We can see that $\prob{A|B}=\frac{\prob{A\cap B}}{\prob{B}}$, and also
that $\prob{B|A}=\frac{\prob{B\cap A}}{\prob{A}}$.  These relations
are called the _law of conditional probability_.  Noting that $A\cap
B\equiv B\cap A$, and rearranging, we arrive at *Bayes's theorem*:

\begin{equation}
\frac{\mbox{$\prob{A|B}$}}{\prob{A}} = 
\frac{\prob{B|A}}{\prob{B}}
\end{equation}

In practice, we re-write $\prob{B}$ and rearrange.  It should be clear
that $\prob{B} = \prob{B\cap A} + \prob{B\cap\overline{A}}$.  Using
the law of conditional probability once more, we obtain
$\prob{B}=\prob{A}\prob{B|A} +
\prob{\overline(A)}\prob{B|\overline{A}}$.  Rearranging:

\begin{equation}
\prob{A|B} = \prob{A}\frac{
   \prob{B|A}
}{
   \prob{A}\prob{B|A} + \prob{\overline{A}}\prob{B|\overline{A}}
}
\end{equation}

With our example, the left hand side of the above equation is "the
probability that it is raining, given that Robin cycles to work".  The
value of the equation is that all the terms on the right hand side are
available.  Note carefully how the causation operates: the fine
weather *causes* Robin to cycle, and rain *causes* Robin to get the
bus to work, and not cycle.  But we can use the observation that Robin
cycles to *infer* that it was a fine day, on the grounds that if it
was raining he would have caught the bus.


It is easier to appreciate the import of Bayes's theorem if it is
re-stated with different letters:

\begin{equation}
\prob{H|D} = \prob{H}\frac{\prob{D|H}}{\prob{H}\prob{D|H} + \prob{\overline{H}}\prob{D|\overline{H}}}
\end{equation}

Here, $H$ stands for "hypothesis" which explains data $D$.  Here the
hypothesis $H$ would be "it is raining" and of course $\overline{H}$
would mean "it is not raining".  Note that the two hypotheses $H$ and
$\overline{H}$ are exclusive and exhaustive: exactly one of these two
alternatives is true.  The data $D$ would be "Robin cycled to work
today".  In Bayes's theorem we would have the following
interpretations for the various terms:

* $\prob{H}$ This is the *prior* probability that it is raining; that is, the overall probability of raining before observing the data
* $\prob{H|D}$ This is the *posterior* probability that it is raining; that is, the probability of rain, given the data which in this case is the observation that Robin cycled to work that day.
* $\prob{D|H}$. This is the probability of observing the data, given that hypothesis $H$ is correct; in this case it is the probability that Robin cycles, given that it is raining
* $\prob{D|\overline{H}}$. This is the probability of observing the data, given that hypothesis $H$ is incorrect (i.e. that is is fine); in this case it is the probability that Robin cycles, given that it is fine weather.

We can put numbers to this:

* $\prob{H} = 0.2$ a prior probability for rain, from previous experience.  Observe that this imples $\prob{\overline{H}}=0.8$.
* $\prob{D|H} = 0.1$ the probability of cycling given rain.  I generally can't be bothered to cycle if it is wet and get the bus instead.
* $\prob{D|\overline{H}} = 0.5$ the probability of cycling given fine weather.  If it is fine weather, I cycle about half of the time, and half the time I am too tired and get the bus anyway.

We can plug these figures directly in to Bayes's theorem:

```{r}
(0.2*0.1)/(0.2*0.1 + 0.8*0.5)
```


Thus the posterior probability of rain is about 4.7\%.  Note carefully
that the prior probability of rain was 20\% but this has decreased to
4.7\% due to the observation that Robin cycled in to work.
Intuitively, if you see me cycling to work you know it is unlikely
that it is raining.


## Independence

If events $A,B$ satisfy the relationhip $\prob{A}=\prob{A|B}$, then
this tells us that observation of $B$ does not change the probability
of $A$.  We can recast this equation as

\[
\prob{A}\prob{B}=\prob{A\cap B}
\]

that is, to find the probability of both $A$ and $B$ occurring (the
right hand side in the above equation), we simply multiply the
probabilities of $A$ and $B$ occurring separately.  If this is the
case, we say that $A$ and $B$ are *independent*.


## Bayes's theorem as a formal mechanism for updating beliefs

Observe the role of *knowledge* in the preceding discussion.  The
prior probability of rain was my estimate of the probability of rain,
drawing on background meteorology of Auckland.  We update this
probability in the light of data, using Bayes's theorem, to give a
posterior probability that incorporates the observation that Robin
cycled to work.

In the literature you will often see the prior probability called a
*subjective prior* as it reflects features such as hunches, opinion
and other informal reasoning.  Observe carefully that the posterior
contains traces of the subjective prior but is modified by the data.

## Bayes and more than two hypotheses

In the cycling example we had two hypotheses but it is straightforward
to generalize to three.  Suppose we have $H_1$, $H_2$ and $H_3$ being
three hypotheses one of which is true.  Then

\[
\prob{H_i|D} = \prob{H_i}
\frac{\prob{D|H_i}
}{
\prob{H_1}\prob{D|H_1}+ 
\prob{H_2}\prob{D|H_2}+
\prob{H_3}\prob{D|H_3}
}
\]

where $i$ is equal to 1,2 or 3 depending on which of $H_1,H_2, H_3$ we
are interested in.  This may be extended further.  If we have
hypotheses $H_1,H_2,\ldots, H_n$ we would have

\begin{equation}
(\#eq:bayesmultiple)
\prob{H_j|D} = \prob{H_j}
\frac{\prob{D|H_j}
}{
\sum_{i=1}^n
\prob{H_i}\prob{D|H_i} 
}
\end{equation}

and (although the formal mathematics gets a little gnarly) we can have
an infinite number of hypotheses indexed by a parameter $x\in X$:

\[
\prob{H_y|D} = \prob{H_y}
\frac{\prob{D|H_y}}{\int_{x\in X} P(x)\prob{D|H_x} dx}
\]

where $H_y$ is some particular hypothesis we are interested in, and
$P(x)$ is the prior probability density function of $H_x$.  We will
see examples of this later in this chapter but for now we can use the
observation that the denominator is a constant which does not depend
on $y$ and restate Bayes as:

\begin{equation}
(\#eq:propbayes)
\prob{H_y|D}\propto\prob{H_y}\prob{D|H_y}
\end{equation}

### Independence and Bayes

If $D$ and $H$ are independent, we would have $\prob{H} = \prob{H|D}$.
Informally, this means that the prior probability of $H$ is equal to
its posterior probability.  Thus observation of $D$ does not change
the probability of $H$ occurring.  We say that $D$ is *uninformative*
about $H$.

## Bayes and the beta distribution


```{block2, type='wikipedia'}
<https://en.wikipedia.org/wiki/Beta_distribution>
```

The *beta* distribution is often used when we are discussing a
sequence of Bernoulli trials and wish to make inferences about $p$,
the probability of success.  One difficulty in dealing with
probabilities is that probability must be non-negative, and cannot
exceed 1; we have $0\leq p\leq 1$, and this fact means that we cannot
use distributions such as the Gaussian which is infinitely wide.

The only reasonable distribution for probabilities is the beta
distribution; see `?rbeta` for details on R's functionality.
The probability density function for the beta is

\begin{equation}
P(p) = \frac{\Gamma(\alpha+\beta)}{\Gamma(\alpha)\Gamma(\beta)}p^{\alpha-1}(1-p)^{\beta-1}
\end{equation}

In the above equation, $\alpha$ and $\beta$ are parameters [cf the
Gaussian, which has parameters $\mu,\sigma$] and $p$ is a probability,
and we understand $0\leq p\leq 1$.  We also introduce the gamma
function $\Gamma(\cdot)$ which is defined as $\Gamma(x)=(x-1)!$; get
help on this by typing `?gamma` at the R prompt.  We can choose
different values for $\alpha$ and $\beta$ to change the shape of the
distribution in much the same way as we choose the mean and variance
in the Gaussian.

```{r}
p <- seq(from=0,to=1,len=100)
plot(p,dbeta(p,shape1=2.1, shape2=5.5),type='l')
```

(note that R uses shape1 and shape2 in place of the more common
notation $\alpha,\beta$).  In general, high values of $\alpha$ and
$\beta$ corresond to sharply-peaked distributions, and low values
correspond to wider distributions.  We require that $\alpha,\beta>0$
but are otherwise arbitrary.  The figure below shows a variety of beta
distributions; study the R idiom carefully.

```{r}
M <- matrix(c(0.5,0.5, 1,1, 1,3, 4,1, 3,3),ncol=2,byrow=TRUE) #each row=params
p <- seq(from=0,to=1,len=100)  # probability for x-axis
plot(p,p*5,type='n',ylab='PDF',main='the beta distribution')  # setup plot
for(i in seq_len(nrow(M))){  # iterate through rows of M
points(p,dbeta(p, M[i,1], M[i,2]),type='l',lwd=3,col=rainbow(nrow(M))[i]) #draw
}
legend(
"topleft",
legend = apply(M,1,function(x){paste("alpha=",x[1],", beta=",x[2])}),
lwd=2, col=rainbow(nrow(M))
)
```

See how we can represent a wide variety of curves by varying the
parameters; not in particular that the red curve is bimodal.


## Beta distributions as priors

Suppose we wish to make inferences about the probability of success
for a Bernoulli trial.  We are not sure about the value of $p$ but
have some beliefs about what it might be.  We then carry out some
experiments and observe that the experiment succeeds $a$ times and
fails $b$ times out of $n=a+b$ trials; this is our data $D$.  The
Bayesian approach is to use equation \@ref(eq:propbayes) to modify our
beliefs using the data $D$.  We may represent our prior beliefs with a
beta distribution with parameters $\alpha,\beta$.  This distribution
represents our uncertainty about the value of $p$.  To use the
observational data $D$ [$a$ successes and $b$ failures out of $a+b$
trials] we use equation \@ref(eq:propbayes) with a binomial
probability distribution:

\begin{equation}
\prob{p}\propto{a+b\choose a\,\, b}p^a(1-p)^{b}\cdot
\frac{\Gamma(\alpha+\beta)}{\Gamma(\alpha)\Gamma(\beta)}p^{\alpha-1}(1-p)^{\beta-1}
\end{equation}

Clearing the equation of constants we get that the probability density of $p$ is

\begin{equation}
Kp^{\alpha+a-1}(1-p)^{\beta+b-1}
\end{equation}

(here $K$ is a constant of proportionality set so the area under the
curve is one).  This is recognisable as another beta distribution with
different parameters.  By comparing this with the definition of the
beta distribution we find that
$K=\frac{\Gamma(a+\alpha)\Gamma(b+\beta)}{\Gamma(a+b+\alpha+\beta)}$
is the value which ensures that the area under the curve is 1.

Note carefully that in this case both the prior and the posterior are
beta distributions, but with different parameters
($\alpha\longrightarrow\alpha+a$, $\beta\longrightarrow\beta+b$).
This is a very desirable feature of the beta distribution, as the
algebra is straightforward.  In general, the phenomenon of a prior
distribution having the same form as the posterior is known as
"conjugate prior".


### Example.

Suppose we have a prior of $\alpha=1.3, \beta=1.7$ as a prior for $p$.
This represents our opinion about the likely values of $p$.  We
conduct 10 Bernoulli trials and observe 3 successes and 7 failures.
Then the posterior distribution for $p$ will be beta with parameters
$\alpha+a=1.3+3=4.3$ and $\beta+1.7+7=8.7$:


```{r}
p <- seq(from=0,to=1,len=500)   # probability on horizontal axis
plot(p,p*0,ylim=c(0,4),ylab="PDF",type="n") # set up plot axes
points(p,dbeta(p,1.3,1.7),lwd=2,type="l",col='black') # prior
points(p,dbeta(p,1.3+3, 1.7+7),lwd=2,type="l",col='red') # posterior
legend("topright",lwd=2,col=c("black","red"),legend=c("prior","posterior"))
```

In the above diagram, see how the prior (black) distribution is wide,
reflecting the large uncertainty.  The posterior (red) distribution is
narrow, showing that our data is informative about the value of $p$.

Observe that the Bayesian approach does not provide "the" value of
$p$.  What we have done is to determine a posterior distribution that
reflects our knowledge about it.  We can find the value of $p$ that
maximizes the posterior, or find the mean of the posterior
distribution [see the wikipedia page for how to do this].  We can also
calculate more abstract results; for example, we can find the
posterior probability that $p<\frac{1}{2}$:

```{r betahalf}
pbeta(1/2, 4.3, 8.7)
```


## Further examples of Bayes's theorem.

Bayes's theorem in the form of equation \@ref(eq:bayesmultiple) is
very general.  Here are some other examples of Bayesian reasoning.


### Poisson distribution

Suppose we have a sample of a radioactive source.  It is either
substance A or substance B but we do not know which.  The hypothesis
that it is in fact substance A is written $H_A$ and the hypothesis
that it is in fact substance B is written $H_B$.  Our priors are
$p(H_A)= 0.9$ and $p(H_B)=0.1$.  Our experiment is to observe the
number of counts on a Geiger counter.  Counts are known to be Poisson
and substance A has $\lambda=5.4$ while substance B has $\lambda=7.7$.
Our data is the observed number of counts which is $n=5$.  Bayes's
theorem:

\begin{equation}
p(H_A|D) = \frac{p(H_A)p(D|H_A)}{p(H_A)p(D|H_A)+p(H_B)p(D|H_B)}
\end{equation}

We can use R to calculate the likelihoods:

```{r lalb}
(LA <- dpois(5, 5.4))
(LB <- dpois(5, 7.7))
```

Then Bayes gives us the posterior probability $p(H_A|D)$ as

```{r papb}
prior_A <- 0.9
prior_B <- 0.1
posterior_A <- prior_A*LA/(prior_A*LA + prior_B*LB)
posterior_A
```

So in this case our prior probability $p(H_A)$ was 0.9 and our
posterior $p(H_A|D)$ has increased to about 0.94, on account of the
low count ($n=5$) being more consistent with $H_A$ ($\lambda=5.4$)
than $H_B$ ($\lambda = 7.7$)

### Dice

Men of a certain age will be familiar with different dice used in
board games.  Suppose someone has three dice, d4,d6,d8.  In standard
terminology, "d4" is a four-sided die in the shape of a tetrahedron
with results $\{1,2,3,4\}$, "d6" is a cube that can give
$\{1,2,3,4,5,6\}$, and "d8" is an octahedron with $\{1,2\ldots, 8\}$.
A person takes one of these dice and throws it, obtaining a 5.  We do
not know which of the dice they threw but we have three hypotheses to
consider: $H_4$, $H_6$, and $H_8$.  The observation of "5" is
informative about these via Bayes's theorem.  Suppose our priors are
$p(H_4)=0.1, p(H_6)=0.8, p(H_8)=0.1$.  Then the likelihoods are
$p(D|H_4)=0$ (think about it), $p(D|H_6)=\frac{1}{6}$ and
$p(D|H_8)=\frac{1}{8}$.  Bayes gives us

\begin{eqnarray}
p(H_4|D) = \frac{p(H_4)p(D|H_4)}{p(H_4)p(D|H_4) + p(H_6)p(D|H_6) + p(H_8)p(D|H_8)}=\frac{0.1\cdot 0}{0.1\cdot 0 + 0.8\cdot\frac{1}{6} + 0.1\cdot\frac{1}{8}}=0\nonumber\\
p(H_6|D) = \frac{p(H_6)p(D|H_6)}{p(H_4)p(D|H_4) + p(H_6)p(D|H_6) + p(H_8)p(D|H_8)}=\frac{0.8\cdot \frac{1}{6}}{0.1\cdot 0 + 0.8\cdot\frac{1}{6} + 0.1\cdot\frac{1}{8}}=91\%\nonumber\\
p(H_8|D) = \frac{p(H_4)p(D|H_4)}{p(H_4)p(D|H_4) + p(H_6)p(D|H_6) + p(H_8)p(D|H_8)}=\frac{0.1\cdot \frac{1}{8}}{0.1\cdot 0 + 0.8\cdot\frac{1}{6} + 0.1\cdot\frac{1}{8}}=9\%\nonumber
\end{eqnarray}

and we that Bayes has updated the priors for our three hypotheses and
given three posterior probabilities.
