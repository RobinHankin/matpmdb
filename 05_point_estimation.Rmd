# Point estimation #  {#estimation}


So far we have been considering distributions with *known* parameters.
In the case of the Gaussian distribution we have the mean $\mu$ and
standard deviation $\sigma$.  However, in practice we very frequently
do not know the true values of a parameter.  Consider the binomial
distribution $B(n,p)$ with known $n$ but *unknown* $p$.  We wish to
estimate $p$ from observational data.

Suppose $n=100$ and we observe $r=35$ successes.  We might estimate
that $p=0.35$, on the basis that 35\% of the 100 trials were
successes.  However, observe carefully that, if $p$ really was 0.35,
then we would not observe exactly 35 successes:

```{r}
rbinom(20, 100, 0.35)
```

Recall from the previous chapter that the observations have a mean of
$np=100\times 0.35=35$, and a variance of $100\times 0.35\times
(1-0.35)=22.75$.

However, observe that if the true value of $p$ was $0.36$ then we
might well observe 35 successes.  Thus the observation of 35 successes
does not allow us to distinguish between the two possibilities that
$p=0.35$ and $p=0.36$.

However, the observation of 35 successes means that we would be very
confident in rejecting the suggestion that $p=0.99$.  It is not at all
obvious why $p=0.99$ is "unacceptable" in this sense while $p=0.35$ is
OK.  What we want to do is to find a "best estimate" for the value of
$p$ on the basis of our observation; but also to indicate a range of
uncertainty for our estimate.


## Likelihood ##

```{block2, type='youtube'}
<https://www.youtube.com/watch?v=yeduCyob7DY&list=PL018X5Hlr4RkgE65Pg93TFY-32KCVpW84&index=23>
```

Suppose three people, A, B, and C are discussing the observation of 35
successes out of 100 trials, and wish to make inferences about $p$,
the unknown probability of success.  Person A says that $p=0.2$,
person B says that $p=0.3$, and person C says that $p=0.5$.

To compare these three estimates for $p$, it is natural (after a
while) to calculate the probability of making the observation [of 35
successes out of 100 trials], given that $p$ takes in turn each of the
values specified by the three people.  This is easy in R:

```{r}
dbinom(35,100,p=0.2)
dbinom(35,100,p=0.3)
dbinom(35,100,p=0.5)
```

Think about what this means.  Take the first line: if the probability
truly was 0.2, then the probability of actually seeing the data [35
out of 100 trials] is 0.00019.  If it truly was 0.3, the probability
of seeing the data is about 0.047, and if it truly is 0.5, the
probability would be 0.00086.  Of these three possibilities, 0.3 is
preferable because this has the highest probability.  We say that
$p=0.3$ is more *likely* than the other two guesses.

## The likelihood function

```{block2, type='wikipedia'}
<https://en.wikipedia.org/wiki/Likelihood_function>
```

The above line of reasoning can be streamlined and extended by
allowing $p$, the supposed probability of success, to be *any* value
between zero and one.  It makes sense to plot the probability of
observing the data [35 successes out of 100 trials] as a function of
the supposed probability of success.  This is easy in R:


```{r}
p <- seq(from=0, to=1, len=100)
plot(p,dbinom(35,100,p))
abline(v=0.35)
```

The above plot shows a *likelihood* function: the probability of
seeing the data, as a function of the assumed probability of success.
We can see that the likelihood function is maximized at $p=0.35$^[This
fact can be verified by using differential calculus but this course is
not using calculus].  The graph also shows why $p=0.9$ is a poor
estimate: the likelihood is much smaller than the likelihood for
$p=0.35$.

The *likelihood function* is a general-purpose tool used in many
branches of statistical inference.  It is used when we are considering
a statistical distribution that has one or more unknown parameters.
In this example, the unknown parameter is the probability $p$ of
success.  The likelihood function is a function $\mathcal{L}(\cdot)$
of the unknown parameter, with $\like{p}= \prob{D|p}$.  In English we
say "the probability of seeing the data $D$, given the true value of
the parameter is $p$".  Sometimes we call the parameter the
"hypothesized value", or 'hypothesis" for short and write
$\prob{D|H}$.

The formal definition of likelihood function is a little harder: We
may multiply the likelihood function as described above by an
arbitrary positive value $C$ and come to the same conclusions.  The
wikipedia page gives more details.

For the example of 35 successes out of 100 trials the likelihood
function is

\begin{equation}
\like{p}=C.{100\choose 35}p^{35}(1-p)^{65}
\end{equation}

We can choose the constant $C$ to be any positive number.  If we
choose $C=\frac{1}{{100\choose 35}}$, then the combinatorial term in
the binomial cancels out and we are left with

\begin{equation}
\like{p}=\frac{1}{{100\choose 35}}{100\choose 35}=p^{35}(1-p)^{65}
\end{equation}

which is easier to deal with^[Actually, "*the* likelihood function" is
a slight misnomer, because it is defined only up to a multiplicative
constant.  We really should say "*a* likelihood function", by which we
mean any function that is proportional to $\prob{D|H}$.].  In R it is just 

```
function(p){p^35*(1-p)^65
```

and later when we consider Bayesian analysis we will see how this line
of reasoning allows us to pursue a different form of statistical
inference.


## Likelihood functions for the Gaussian

Another nice example of likelihood functions is given by the Gaussian
distribution.  Suppose we have observations drawn from a Gaussian
distribution $N(\mu,1)$: so we know that the standard deviation is 1,
but we do not know what the mean $\mu$ is, and want to make inferences
about its true value.

Our data is:


```{r}
d <- c(6.1, 6.7, 6.3, 5.7)
```

Of course we could calculate the *sample* mean:

```{r}
mean(d)
```

but what is the uncertainty on this?  The likelihood function is, in R
idiom, easy to calculate.  The idea is that the *data* stays fixed but
allow the putative mean $\mu$ to vary.  Suppose we start with
$\mu=5.8$.  Then the likelihood would be

```{r}
dnorm(6.1,mean=5.8)*
dnorm(6.7,mean=5.8)*
dnorm(6.3,mean=5.8)*
dnorm(5.7,mean=5.8)
```

and we might compare this with the likelihood for $\mu=5.9$:

```{r}
dnorm(6.1,mean=5.9)*
dnorm(6.7,mean=5.9)*
dnorm(6.3,mean=5.9)*
dnorm(5.7,mean=5.9)
```

Comparing these two, we see that $\mu=5.9$ is marginally more likely
than $\mu=5.8$ on the basis that the likelihood is (slightly) higher.
We can plot a likelihood function:

```{r}
like <- function(m){prod(dnorm(d,mean=m))}
m <- seq(from=4,to=8,len=100) # range of plausible population means
plot(m,sapply(m,like))        # see below for why sapply() is used
abline(v=mean(d))             # sample man is the maximum likelihood estimator
```

Study the above carefully: it might look simple, but there is a lot
going on.  Make sure you understand exactly what is happening before
moving on.  Note the use of the ```sapply()``` construction here,
needed to vectorize the ```like()``` function.  See how the likelihood
is maximized at the *sample* mean; this is *why* the sample mean is
calculated.


## The support function

The likelihood function $\like{\cdot}$ is defined as the probability
of obtaining the data, given the hypothesis $H$, multiplied by an
arbitrary constant $C$.

The *support* function $\supp{\cdot}$ is defined as the logarithm of
the likelihood:

\begin{equation}
\supp{H} = \log(\like{H})
\end{equation}

But because the likelihood has an arbitrary multiplicative
constant $C$, the formula is

\begin{equation}
\supp{H} = \log(\prob{D|H}) + \log(C)
\end{equation}

Thus support is a function defined up to an *additive* constant (all
logs are natural logs).  Using the Gaussian example in the previous
section we can plot a support function:

```{r}
support <- function(m){sum(dnorm(d,mean=m,log=TRUE))}  # support function
m <- seq(from=4,to=8,len=100)    # range of plausible population means
plot(m,sapply(m,support))        # plot the support
abline(v=mean(d))                # sample mean is the best-supported value
```

Remembering that we can add or subtract an arbitrary constant, we may
as well ensure that our curve has a maximum at zero:

```{r}
support <- function(m){sum(dnorm(d,mean=m,log=TRUE))}
m <- seq(from=4,to=8,len=100)  # range of plausible population means
s <- sapply(m,support)         # calculate support
plot(m,s-max(s))               # subtract maximum value so support=0 at max
abline(v=mean(d))              # best-supported value is the sample mean
abline(h=0)                    # maximum support = 0
abline(h=-2)                   # two units of support gives credible interval
```


### Credible interval

The two horizontal lines correspond to the maximum likelihood estimate
for the mean, and the line of $\suppval=-2$.  Two units of support is
the standard measure of "a lot of support", so this gives a region of
reasonably well-supported values of the mean that are above the line
$\suppval=-2$; this is known as a *credible interval*.  From the
graph, the credible interval is from about 5.2 to 7.1.

The way to think about this is to ask how much "extra" support one can
achieve from any given starting point.  Suppose one started at
$\mu=4$.  On the graph above, we can see that this has a support of
about -10, which would correspond to a likelihood of $e^{-10}\simeq
4.54\times 10^{-5}$.  Then by moving from $\mu=4$ to $\mu=6.2$ we can
slide up the support curve to the maximum point, and thereby achieve
an *extra* 10 units of support.  This is more than the
2-units-of-support criterion, so we can be reasonably sure that
$\mu=4$ is not the true value; but $\mu=6$ is acceptable.

The point at which the support curve is maximized is the "maximum
likelihood estimate", also sometimes called the "evaluate".


## Note on likelihood and support as relative measures of credibility

The arbitrary constant $C$ in the definitions of support and
likelihood reminds us that only *relative* values of support and
likelihood are meaningful.  You cannot calculate "the support" of any
single hypothesis; the concept is only meaningful when comparing two
hypotheses.

Going back to our example of 35 successes out of 100 trials, we have
been calculating the likelihood as $p^{35}(1-p)^{65}$, and the support
will be $35\log p+65\log(1-p)$.  However, as stated above we are only
interested in relative changes of support, and relative support
support is unchanged by addition of an arbitrary constant $C$.  The
diagram below shows a number of support curves:


```{r}
p <- seq(from=0.1,to=0.5,len=100)  # set up horizontal axis
plot(p,p*0,type='n',ylab='support',ylim=c(-70,-55))  # set up axes

for(i in 1:10){  # loop over i
  points(p,35*log(p)+65*log(1-p)+i,col=rainbow(10)[i],type='l')  # plot curves
}
```			

Observe how the actual value of the y-axis is immaterial.  One would
make the same inferences from any of the support curves shown, which
is why the arbitrary constant $\log C$ may be added to the support
function.

## Bias


```{block2, type='wikipedia'}
<https://en.wikipedia.org/wiki/Bias_of_an_estimator>
```

Suppose we want to estimate the standard deviation $\sigma$ of a
population from $n=5$ observations from a standard Gaussian $N(0,1)$,
and we use R idiom ```sd()``` to do so:

```{r}
sd(rnorm(5))
```

Observe that in the above, we actually know that $\sigma=1$ but the
estimate is not exactly correct, due to sampling error.  We might ask
what happens if we repeatedly estimate $\sigma$:

```{r}
replicate(30,var(rnorm(5)))
```

In the above, observe that some figures are overestimates and some are
underestimates.  Of course, in practice we only have a single number
to work with, and in general we will not know whether it is an
overestimate or an underestimate.  Statisticians use a "hat" over a
symbol to denote an estimate; thus "$\hat{\sigma}$" means an
estimated value of $\sigma$.  Using R, we can draw a histogram of
estimates $\hat{\sigma}$:

```{r}
hist(replicate(10000,sd(rnorm(5))),col='gray')
abline(v=1,lwd=6)
```

in the above we have drawn the correct value of the standard deviation
$\sigma=1$ for convenience.  See how the distribution of estimates
includes severe overestimates and severe underestimates, both caused
by the finite sample size (5 in this case).  We might ask what the
*expected* value is of $\hat{\sigma}$:

```{r}
mean(replicate(10000,sd(rnorm(5))),col='gray')
```

So the mean of the estimates $\overline{\hat{\sigma}}=0.93$ is quite
far from the true value of $\sigma=1$.  The *bias* of an estimator is
defined as the difference between the expected value of the estimator
and the true value of the parameter.  The bias of ```sd()``` is
difficult to calculate exactly but in this case its numerical value is
about $0.93-1.00=-0.07$.

### Bias for variance

(this is harder than the preceding material and should be viewed as
optional reading).  We might estimate the variance of a distribution
by calculating the mean value of the squared deviance:

```{r}
myvar <- function(x){
  deviance <- x-mean(x)
  return(mean(deviance^2))
}
```

This would correspond to

\[
\frac{1}{n}\sum_{i=1}^n\left(x_i-\overline{x}\right)^2
\]

and we can see the bias of this estimator:

```{r}
mean(replicate(10000,myvar(rnorm(5))))
```

If we denote the value of ```myvar(d)``` by $M$, it can be shown that
$\mathbb{E}\left(M\right)=\frac{n-1}{n}\sigma^2$.  We can correct for
this by defining $\tilde{M}=\frac{n}{n-1}M$ which is why you see $n-1$
(instead of $n$) as a denominator in the literature.  Your lecturer
has spent his entire life thinking about bias of estimates.  He does
not think that bias is a useful or informative thing to calculate and
believes that pursuit of unbiased (that is, a bias of zero) estimators
is pointless, counter-productive and indeed actively misleading.
