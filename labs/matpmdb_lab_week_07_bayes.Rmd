---
output: html_document
title: Priors and posteriors in the beta distribution
fontsize: 12pt
---
# Bayes's theorem


\newcommand{\prob}[1]{\operatorname{Prob}\left(#1\right)}


\section{The Beta distribution}

You will recall from lectures that the beta distribution is
proportional to $p^{\alpha-1}\left(1-p\right)^{\beta-1}$
for $0\leqslant p\leqslant 1$.

Look at the R help page for the beta distribution by typing 
  `?rbeta`

We will sample from this distribution and play with it.

* choose different values of alpha and beta and examine the
  resulting histogram.  Use R idiom like `hist(rbeta(10000,2,5)}`.
  Try $\alpha=2,\beta=2$,
  $\alpha=4,\beta=4$, $\alpha=12,\beta=12$.  Try increasing the
  parameters and see what happens. See how the distribution is
  symmetric about $p=0.5$.
* Now try $\alpha=1,\beta=2$, $\alpha=3,\beta=6$, etc.  The
  mean value of the beta distribution is known from theory to be
  $\frac{\alpha}{\alpha+\beta}$.  Is this reasonable?
* Now try to verify the formula for the variance, given in
  the infobox on the wikipedia page.
* Do the same for the median (use the approximate formula; use either
  ` median(rbeta(...))` or (better; harder) `qbeta(0.5,alpha,beta)}`.
  Try a range of alpha and beta values.


\section{Analytical formula}

We can plot the analytical form of the beta distribution using `dbeta()` which gives the density of the beta distribution:

```{r} 
p <- seq(from=0,to=1,len=300)
plot(p,dbeta(p,shape1=1.2,shape2=1.1),type='l',ylim=c(0,3))
points(p,dbeta(p,shape1=2,shape2=2),col="red",type='l')
points(p,dbeta(p,shape1=1,shape2=3),col="green",type='l')
points(p,dbeta(p,shape1=5,shape2=3),col="blue",type='l')
points(p,dbeta(p,shape1=0.3,shape2=0.6),col="yellow",type='l')
points(p,dbeta(p,shape1=2,shape2=0.2),col="orange",type='l')
```

Try to replicate the first figure on the wikipedia page.

Pay special attention to what happens when $0\leqslant\alpha,\beta\leqslant 1$ and interpret.


## Analytical formula

In lectures we gave an equation proportional to the density:

    
$$\prob{p}\propto p^{\alpha-1}\left(1-p\right)^{\beta-1}$$


The actual formula is 

$$\prob{p}=\frac{\Gamma(\alpha+\beta)}{\Gamma(\alpha)\Gamma(\beta)}\cdot
        p^{\alpha-1}\left(1-p\right)^{\beta-1}$$

where $\Gamma(x) = (x-1)!$ = ``factorial $x-1$'' is the gamma function.
Look at the wikipedia page for the gamma function, and the R help page
(type `?rgamma`).  This term is present to ensure that the area
under the graph is exactly one.

Verify for yourself that this formula is correct by comparing it with
R's internal function, `dbeta()`.


```{r}
dbeta(0.2,2.1,4.1)
gamma(2.1+4.1)/(gamma(2.1)*gamma(4.1))*0.2^1.1*0.8^3.1
```

Try other values of $\alpha$ and $\beta$ and compare.  Which evaluation is
more accurate?

# Prior and posterior PDF


Now consider a case where we have a Bernoulli experiment with unknown
probability of success, $p$.  Our prior is a beta distribution with
$\alpha=1.3, \beta=2$ and our data is 10 experiments with 7 successes
and 3 failures.  The posterior distribution will also be a beta
distribution with $\alpha=1.3+7=8.3$ and $\beta=2+3=5$.

We can plot these:
    
```{r} 
p <- seq(from=0,to=1,len=300)
plot(p,dbeta(p,shape1=1.3,shape2=2),type='l',lwd=3,ylim=c(0,3))
points(p,dbeta(p,shape1=8.3,shape2=5),col="red",type='l',lwd=3)
legend("topright",col=c("black","red"),legend=c("prior","posterior"),lty=1,lwd=3)
```

Your task:
    
* Try different priors, such as alpha=10,beta=11, and see what
happens.
* Try different datasets, such as 100 experiments with
71 successes, and see what happens.

* (harder).  Try alpha=0.1, beta=0.1 with the following sets of data:  (a) 100 trials, 100 successes; (b) 100 trials, 99 successes; (c) 100 trials, 1 success; (d) 100 trials, 50 successes.  Interpret.

  
# Elicitation

The following examples are typical of the statistical consulting process.  The informants' quotes closely resemble what actual people say when I need a prior.  Note the usage of perfectly sensible English and the difficulty of translating this into mathematical statements.

In each case, $p$ represents the probability of success.
 
 
## exercise 1
  
An informant has beliefs about $p$, and you are trying to  represent these beliefs in terms of a beta distribution. He says  ``I don't know what p is exactly.  It's probably about 0.35, but I'm not sure.  I'd say it's most likely not less than about 0.2  and I'd be surprised if it was more than about 0.6''.

Translate this rather vague information into a statement about the parameters of a beta distribution.  This is the prior PDF.  Then imagine that your data is $n=10$ trials with $r=2$ successes.  Calculate and plot your posterior PDF.

## exercise 2    

(harder) Suppose your informant expresses his prior beliefs in the following terms: "I don't know what p is, but I reckon it's unlikely to be moderate; it is almost certainly not in the range 0.25-0.75.  It is either either close to zero or close to one.  I'd say it's a bit more likely to be close to zero than it is to be close to 1.  But I wouldn't be surprised if it was 0.95.  If it was 0.4 or 0.5, I'd be astonished.  I'd expect values of say 0.03 or 0.07, or on the other hand 0.94 or 0.99 even.  But it won't be in mid-range.  It's not a fair coin.  It's strongly biased, and I don't know which way''.
    
An experiment is performed twice, with two successes.  Give a reasonable prior and a reasonable posterior distribution for $p$.

## exercise 3


(much much harder)  Your informant states:  ``I don't know what p is, but I can definitely rule out the ranges 0-0.1, 0.2-0.5, and 0.8-1.    So it must be either 0.1-0.2 or 0.5-0.8.   I'd say it is twice as likely to be in the low range as the high range.  

A single experiment is performed which succeeds.  State your prior and posterior PDF [you may assume that the prior PDF is uniform in both ranges, although the density might be different in the two ranges]

Hint 1: state Bayes's theorem, and think carefully about what the denominator means.

Hint 2: consider $H_L$, which is that $0.1\leqslant p\leqslant 0.2$, and $H_U$, $0.5\leqslant p\leqslant 0.8$.  State prior probabilities for $H_L$ and $H_U$.
