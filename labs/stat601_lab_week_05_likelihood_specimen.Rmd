---
title: "Likelihood and the Gaussian distribution"
output:
  html_document:
    df_print: paged
fontsize: 12pt
---

# Likelihood and the Gaussian distribution

This short handout takes you through the basics of likelihood as
applied to the Gaussian distribution and the binomial distribution.
Go through the material, typing the R commands at the R prompt, making
sure you understand the content.
       
Remember that you can type `?rnorm` at the R prompt to get help
on that command.
       
Then see the figure for a graph.
    
```{r}
x <- seq(from=-4,to=4,len=100)
plot(x,dnorm(x),pch=16,ylim=c(0,0.6),type='l',main='some Gaussian distributions')
points(x,dnorm(x,mean=1),col="red",type='l')
points(x,dnorm(x,mean=2),col="green",type='l')
points(x,dnorm(x,sd=2),col="blue",type='l')
points(x,dnorm(x,sd=0.7),col="yellow",type='l')
points(x,dnorm(x,mean=1,sd=1.1),col="orange",type='l')
abline(v=0,lwd=5,col='purple')
```

If the purple line represents an observation, which is the best Gaussian curve to choose?


# Likelihood

You will recall that the likelihood function is defined for a particular dataset $D$ as

$$
\mathcal{L} = C\times\operatorname{Prob}\left(D\left|H\right.\right)
$$

where $C$ is an arbitrary constant, $D$ a (fixed) dataset, and $H$ a hypothesis that we use to describe the data (such as a particular value for the mean and standard deviation of a Gaussian distribution, or the parameter $p$ in a binomial distribution).

In lectures we considered the Nile dataset.  Today we are going to
consider only the first 20 measurements:

```{r}
D <- Nile[1:20]
``` 

Assuming that the dataset has a Gaussian distribution we may calculate
the probability of observing this dataset, conditional on specific
parameters for the mean and standard deviation, with `dnorm()`:
    
    
```{r}
dnorm(D,mean=900,sd=140)
```

See how the values are quite small because of the high (140) standard
deviation.  To get the overall probability  (which would be a likelihood), we multiply all the
individual probabilities together with `prod()`:

    
```{r}
prod(dnorm(D,mean=900,sd=140))
```


And we may investigate the effect of changing the mean:


```{r}
prod(dnorm(D,mean=902,sd=140))
```

so the likelihood has increased, which tells you that 902 is a better
estimate for the mean than 900.

We can also investigate changing the standard deviation:

```{r}
prod(dnorm(D,mean=902,sd=141))
```

Observe that we do not care about the absolute value of the
likelihood; we only care about relative changes.


**YOUR TASK** play around with different values of the mean and
standard deviation until you find the *maximum likelihood estimate*

```{r}
prod(dnorm(D,mean=902,sd=141))
prod(dnorm(D,mean=902,sd=151))
prod(dnorm(D,mean=902,sd=161))
prod(dnorm(D,mean=902,sd=171))
prod(dnorm(D,mean=1070,sd=143))
```



## Graphical methods

See the following figure, in which vertical lines correspond to the
data.  Which colour curve corresponds to the highest likelihood?

```{r}
D <- Nile[1:20]
x <- seq(from=800,to=1500,len=100)
plot(x,dnorm(x,mean=850,sd=100),type="l",lwd=3)
points(x,dnorm(x,mean=950,sd=100),type="l",lwd=3,col='red')
points(x,dnorm(x,mean=950,sd=140),type="l",lwd=3,col='green')
points(x,dnorm(x,mean=1300,sd=190),type="l",lwd=3,col='blue')
segments(x0=D,y0=0,y1=0.004)
```

It makes sense to *plot* the likelihood when changing one or other
parameter.  The following graph shows what happens when you change the
mean.


```{r}
f <- function(m){prod(dnorm(D,mean=m,sd=141))}
m <- seq(from=900,to=909,len=100)
plot(m,sapply(m,f))
```

(above, the `sapply()` function is just R idiom to pass the elements
of `m` to the function `f()`).  It is more common to work with the log
of the likelihood.  The professional uses the fact that
$\log\prod_{i=1}^nx_i=\sum_{i=1}^n\log x_i$, so uses the `log`
argument of `dnorm()`:

```{r}
f2 <- function(m){sum(dnorm(D,mean=m,sd=141,log=TRUE))}
f2(900)
```

This is more accurate, and less susceptible to overflow.

**YOUR TASKS**: play with the R idiom for these two figures.
 
 
* For dataset D plot a likelihood function for the mean but use a
  sensible range.  Then vary the standard deviation from 141.
* Write R idiom for plotting the likelihood as a function of standard
  deviation, holding the mean constant.
* Try including the whole Nile dataset instead of just the first few
  observations.
* (harder) The following dataset is drawn from a Gaussian distribution:

   `1.1, 1.2, 4.6`.

Calculate a likelihood function for the hypothesis that the mean is
$(1.1+1.2+4.6)/3=2.3$ but the standard deviation is between 0.3 and 5.

# Likelihood maximization

R can perform maximization automatically using the `optim()` function.
Unfortunately, this minimizes its objective function rather than
maximizing (which is a pain).

We will also work with the log likelihood, as this is more convenient
for numerical work:
    
```{r}
D <- Nile[1:10]
log(prod(dnorm(Nile,mean=900,sd=150)))
sum(log(prod(dnorm(Nile,mean=900,sd=150))))
sum(dnorm(Nile,mean=900,sd=150,log=TRUE))
```

Make sure you understand the above logic.  All the lines calculate the
same quantity using different R idiom.  

In the following, we define a vector `p` with two elements, the first
being the mean and the second the standard deviation.  The `optim()`
function is minimizing `f()` which is *minus* the
log-likelihood:

```{r}
f <- function(p){-sum(dnorm(Nile,mean=p[1],sd=p[2],log=TRUE))}
optim(c(900,150),f)
```

In the above R session, the output from `optim()` splits into
different parts.  The one we want is `$par`, which gives the value
that maximizes the objective function `f()`.  The first argument to
`optim()` is the start point for the optimization routine.


This shows us that the maximum likelihood estimate is 919.3 for the
mean and 168.4 for the standard deviation.

**YOUR TASK** is to adapt the previous R idiom to work with random
observations from a standard Gaussian.  To ensure everyone has a
different dataset, we will use our student ID as a seed.  In the
following, I have used my student ID, you replace this with your ID.
    
    
```{r}
set.seed(1266402)  # my student ID: you use yours!
myRandom <- rnorm(10)
f <- function(p){-sum(dnorm(myRandom,mean=p[1],sd=p[2],log=TRUE))}
optim(c(0,1),f)
```


I want everyone to do this and report to me the maximimum likelihood
estimate.  The tutor will collate them.

Further, I want everyone to evaluate the likelihood of the *TRUE*
values of the parameters:


    
```{r}
set.seed(1266402)  # my student ID: you use yours!
myRandom <- rnorm(10)
f <- function(p){-sum(dnorm(myRandom,mean=p[1],sd=p[2],log=TRUE))}
f(c(0,1))   # negative log-likelihood for mean=0, sd=1 
```


## Binomial distribution.

Suppose we have some observations of independent Bernoulli trials.  We
observe, say, 16 trials of which 5 are failures and 11 successes.
What can we say about $p$, the probability of success?

By definition, the likelihood function is just
$C\operatorname{Prob}\left(D|H\right)$, where $D$ is the data (here 11
successes and 5 failures) and $H$ is the hypothesis that the
probability of success is a particular value $p$ which we can supply.
The first thing we can do is to state that the likelihood function is

$$
\mathcal{L(p)}=C{16\choose 11\,\,\,\,5} p^{11}\left(1-p\right)^{5}
$$

But we can choose $C={16\choose 11\,\,\,5}^{-1}$ to give us just
$p^{5}\left(1-p\right)^{11}$.  But in practice it is often easier to
choose $C=1$, which enables us to use `dbinom()` directly:

```{r}
like <- function(p){dbinom(11,16,p)}
p <- seq(from=0,to=1,len=100)
plot(p,dbinom(11,16,p),xlab='probability of success',ylab='likelihood')
abline(v=11/16)
```

The *support* $\mathcal{S}$ is just the log of the likelihood function.  Here we have 

$$
\mathcal{S}=\log\mathcal{L} = \log\left(C{16\choose 11\,5}p^5\left(1-p\right)^{11}\right)=\log K+5\log p+11\log\left(1-p\right)
$$

where $K=\log C+\log{10\choose 11\,5}$. In this case it makes sense to choose $K$ so the support has a maximum of zero.  We know that the maximum likelihood estimate is $\hat{p}=\frac{11}{16}$, so if we subtract off the probability at that point we have a support function that has a maximum of zero.  Here we will use `dbinom(...,log=TRUE)` rather than mucking about with logs:


```{r}
p <- seq(from=0.2,to=0.9,len=100)
support <- function(p){dbinom(11,16,p,log=TRUE) - dbinom(11,16,11/16,log=TRUE)}
plot(p,support(p))
abline(h= -2)
abline(v=11/16)
```

If we want to give a support interval we find where the support curve
intersects the line at -2.  The values are about 0.4 and 0.9 but to
find them exactly we can use numerical methods (solving the equations
analytically is difficult).  In R the way to solve an equation
numerically is to use `uniroot()`.  This takes two arguments, a
function and a two-element vector bracketing the root.  `uniroot()`
solves $f(x)=0$ and we want $S=-2$ so we need $f(x)=S+2$ because then
$f(x)=0$ means $S+2=0$, ie $S= -2$.

```{r}
f <- function(p){support(p)+2 }  #uniroot solves f(x)=0 so S+2=0
uniroot(f,c(0.3,0.5))       # second argument brackets the root
```

So the lower limit is about 0.439 (this is given in the `$root` line).  Finding the upper limit is analogous:

```{r}
uniroot(f,c(0.8,0.95))
```

giving an upper limit of about 0.877.  So a support interval might be
$(0.439,0.877)$.  Inside this range, you cannot improve the likelihood
by two units of support.  So this range is OK.  Outside this range,
you can find a better estimate for $p$ that is much better (better by
more than two units of support) simply by moving to the maximum
likelihood estimate of $11/16$.

**YOUR TASK**

Perform the same calculations with the following data:

* 2 trials, 1 success and 1 failure
* 20 trials, 10 successes and 10 failures
* 80 trials, 40 successes and 40 failures (compare these three with
  one another and observe the effect of having more observations)
  
```{r}
ps <- function(s,f,...){
  p <- seq(from=0.01,to=0.99,len=500)
  supp <- dbinom(s,s+f,p,log=TRUE)
  plot(p,supp-max(supp),type="l",...)
  abline(h=c(0,-2))
}

ps(1,1,ylim=c(-5,0))
ps(10,10,ylim=c(-5,0))
ps(40,40,ylim=c(-5,0))
```

* 2 trials, 2 successes
* 10 trials, 10 successes
* 20 trials, 20 successes

```{r}
ps(2,0,ylim=c(-5,0))
ps(10,0,ylim=c(-5,0))
ps(20,0,ylim=c(-5,0))
```

Interpret these using non-statistical language (the trials may be
assumed to be independent).

More data gives tighter estimates.

(harder).  Suppose we conduct 5 repeated independent Bernoulli trials
with probability $p$ of success.  Our observation is that "one or two
successes were observed".  Plot a likelihood function for $p$.

```{r}
p <- seq(from=0.01,to=0.9,len=100)
L <- dbinom(1,5,p) + dbinom(2,5,p)
S <- log(L)-log(max(L))
plot(p,S,type="b",ylim=c(-6,0))
abline(h=c(0,-2))
```

(even harder).  An experiment is performed: independent Bernoulli
trials are performed until a total of two successes are observed, at
which point the experiment stops.  The observations are FFFSFFFFFFFS.
Write down a likelihood function for $p$.  Firstly, test the
hypothesis that $p=0.5$ using likelihood methods; and secondly test
the null hypothesis that $p=0.5$ by giving a $p$-value (you can use
numerical simulation for this, or utilise the inverse binomial
distribution if you find that easier).  Compare these two answers and
methods.

Numerical:

```{r}
p <- seq(from=0.1,to=0.4,len=30)
num <- function(p){min(which(cumsum(rbinom(100,1,p))==2))}
like <- function(p){sum(replicate(1000,num(p))==12)}
L <- sapply(p,like)
plot(p,log(L/max(L)))
```

A bit wobbly.

```{r}
S <- dnbinom(12,2,p,log=TRUE)
plot(p,S-max(S))
```
