---
output: html_document
title: Nonparametric methods
fontsize: 12pt
---


# Introduction

This short handout takes you through a few nonparametric techniques.
Recall from lectures that nonparametric methods make no assumptions
about the distribution from which observations are drawn.  They are
useful if one cannot assume normality, for example, and are generally
more robust than parametric techniques such as the student t test
(which assumes normality).


Remember that you can type `?qqnorm` at the R prompt to get help
on that command.

## Example 1


A metrology scientist uses two instruments, "a" and "b", to estimate
the mass of a standard kilogram.  The following two datasets show the
difference between the measured mass and its true value, in
micrograms:

```{r}
a <-   c(-0.15, 0.97, -1.8, -1.33, 2.97, -1.16, -3.02, 2.18, 0.28, 
       -1.55, -0.04, 1.16, 0.17, -1.24, -0.13, -2.49, 0.92, -1.73, -1.94, 
       0.69, -0.47, 1.06, -0.82, 0.18, 0.26, -1.04, 0.33, -0.55, -1.68)
b <- c(-3.2, 0.29, -5.74, -3.51, 6.11, 20.43, 10.66, 4.54, -0.93, 
      0.78, 2.39, 3.39, -0.25, -7.32, 3.38, -7.37, 13.8, -9.61, 3.05, 
     -2.05, -1.2, -1.06, -3.55, 2.95, 10.23, -3.58, 8.7, 4.02, -7.95)
```

Observe that `t.test(a)` and `t.test(b)` return non-significant
p-values (try it!): there is no evidence that the mean of the datasets
is different from zero.

### Your tasks

* Estimate the mean and standard deviation of these two datasets
* Using `qqplot()`  plot a quantile-quantile
  plot of `a` against `b`; use `abline(0,1)` to plot a line of equality
* Using `t.test()`, perform a Student t test on
  these two datasets
* Using `ks.test()`, perform a
  Kolmogorov-Smirnov test on the two datasets
* Using `wilcox.test()`, perform a Wilcoxon
  rank sum test on the two datasets
* By considering the null of each test, explain why two of the
  tests return non-significant p-values and one returns a significant result
* Interpret your p-values in such a way that a metrologist who
  (due to gross organizational incompetence) is not a statistician,
  could understand

## Example 2

Consider samples of size 5 from two unit-variance normal
distributions: one with mean zero, one with mean 1.  The R idiom would
be `rnorm(5,mean=0)` and `rnorm(5,mean=1)`.  We can test the null of
equality of means with `t.test()` or `wilcox.test()`:

```{r}
t.test(rnorm(5),rnorm(5,mean=1))
```

And we can extract the pvalue on its own with
`t.test(rnorm(5),rnorm(5,mean=1))`.  We can replicate this, say 1000
times using \verb+replicate()+:

```{r}
x <- replicate(1000,t.test(rnorm(5),rnorm(5,mean=1))$p.value)
```

### Your tasks 


* Carry out the above replicated test and, using R constructions
  like `table(x<0.05)`, assess the power of the t test for normal
  data with mean 1.  Recall that "power" means "probability of
  correctly rejecting the null hypothesis when it is incorrect".

* You will recall that it is possible to generate random variables
  drawn from the Cauchy distribution with constructions like
  `rcauchy(5,loc=1)`.  Repeat the above analysis, with samples of size
  5 but this time drawn from Cauchy distributions with location zero
  and 1 respectively.  What is the power of the t test under these
  circumstances?

* By this criterion, is the t test performing better or worse when the
  samples are Cauchy?  Briefly state why you might expect this answer.



## Example 3

This question asks you to compare using the sample mean and the median
  as estimators of the mean.  The following R idiom
  
```{r}
mean(rnorm(10))
```
  
generates 10 normal observations and calculates their mean.  Call
this "estimator 1".  Then we can   calculate estimator 1 a total of 30 times:

```{r}
est1 <- replicate(30,mean(rnorm(10)))
est1
```

### Your tasks
  
* What is the variance of this estimate of the mean? (you will need
    more than 30 replicates to get a sensible estimate)
* We will define "estimator 2" to be `median(rnorm(10))`.
    What is the variance of estimator 2?
* In this context, the variance is a good measure of the
    performance of an estimator.  Which estimator has a lower
    variance?  Which is the better estimator?
* Carry out the analysis above, but sampling from a Cauchy
    distribution (use `rcauchy()`).  Which is the better
    estimator now?
