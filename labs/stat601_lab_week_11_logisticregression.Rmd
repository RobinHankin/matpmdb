---
output: html_document
title: logistic regression
fontsize: 12pt
---

# Logistic regression


Consider the following dataset, taken from the discipline of human
psychology, in which a person is asked to identify whether or not
there is a particular feature in an image that is presented to the
subject.

```{r}
y <- c(F,F,T,F,F,T,T,T,F,F,T,T,T,F,T,T)
```

Dataset y is the time in seconds that the subject is allowed to
view the image, and ``y'' is the result, with 0 meaning ``incorrect''
and 1 meaning ``correct''.

We can plot this easily:


```{r}
x <- seq_along(y)
plot(y~x,pch=16)
```

# Doing logistic regression by hand

We can attempt to find the logistic fit by hand.  First we need to
define a logistic transformation:

```{r}
logistic <- function(x){exp(x)/(1+exp(x))}
LO <- function(p){log(p/(1-p))}
```


```{r}
p <- runif(100)
max(abs(p-logistic(LO(p))))
```

and

```{r}
z <- rnorm(100)
max(abs(z-LO(logistic(z))))
```

Above, we see verification of `LO()` and `logistic()`, which are
inverses of one another.  Make sure you understand what these lines
do, and why we need both of them.  We can now use these functions to
plot the regression line.


First of all we need a likelihood function.

```{r}
like <- function(beta){
  p <- logistic(beta[1] + beta[2]*x)
  success <- y==1
  fail    <- y==0
  prod(p[success]) * prod(1-p[fail])
}
```

(See how `like()` is a direct R implementation of the likelihood
function under investigation).  We can evaluate this at a couple of
points:

```{r}
like(c(1,1))
like(c(1,2))
```

So we have a direct comparison of $H_1\colon\beta_1=1,\beta_2=1$ and
$H_2\colon\beta_1=1,\beta_2=2$ by way of a likelihood ratio:

```{r}
like(c(1,1)) / like(c(1,2))
```

In this case, $H_1$ is astronomically more likely than $H_2$.  But of
course we are free to choose $\beta_1,\beta_2$ as we wish, and can use
optim() to find the MLE:


```{r}
f <- function(beta){-log(like(beta))}
optim(c(1,1),f)
```

(a more mathematical approach would show that we do not need to worry
about local maxima).  We can plot the regression line using these
values:

```{r}
plot(x,y,pch=16)
xx <- seq(from=0,to=16,len=100)
points(xx,logistic(-1.34 + 0.193*xx),type="l") # numbers from optim() result above
```

# Logistic regression using glm() 

We can get more information and a more sophisticated analysis using
built-in R funcitonality.  The regression is carried out using the
`glm()` function:


```{r}
fit <- glm(y~x,family='binomial')  #  "binomial" tells glm() that 'y' is Bernoulli
fit
```

And we can see more detail using summary():

```{r}
summary(fit)
```

So, from the pvalue we can see that the relationship is not
significant.  We can plot the regression using the logistic transform
function.  Taking the coefficients from the fit, we have:
    
```{r}
plot(y~x,pch=16)
points(x,logistic(-1.34 + 0.19*x),type='l')
```

And this would show that the estimate for the value of x that gives
$p=0.5$, that is $\hat{x} = -B/A = 1.34/0.189=7.1$, is about right.
Function `glm()` allows one to use the full power of the function
specification and we are free to use multivariate regression, etc etc.


# Synthetic example


Here I show how to generate observations from a process in which the
assumptions of logistic regression are known to be exactly correct.
This is very similar to constructions from the first part of the
course such as `mean(rnorm(100))`, which was used to study properties
of the sample mean of iid Gaussian RVs.


```{r}
set.seed(0)
n <- 50
x <- seq(from = -3,to=3,len=n)
alpha <- 0.3
beta <- -1.2
p <- logistic(alpha + beta*x)  # probability of success
y <- rbinom(n,1,p)
fit <- glm(y~x,family="binomial")
summary(fit)
jj <- fit$coefficients
alpha_fit <- jj[1]
beta_fit <- jj[2]

plot(x,y,pch=16)
points(x,logistic(alpha + beta*x),type="l",col="red")   # true values
points(x,logistic(alpha_fit + beta_fit*x),type="l",col="blue") # fitted values
legend("topright",lty=1,col=c("red","blue"),legend=c("truth","fitted"))
```


Notes:

* This time the probability of success _decreases_ with increasing x; the coefficient is negative.
* The assumptions of `glm()` are met _exactly_ and we have a large amount of perfect data
* The fit is not particularly close as the red and blue curves are quite distinct
* The standard errors of the coefficients are quite large but the confidence interval
     should include the true values (can you say with what probability this is so?)

take-home message: logistic regression is easy but requires a lot of
data to reject your null.


# Your Tasks

* Consider the following dataset: `y <-
  c(F,F,T,F,F,T,T,T,F,F,T,T,T,F,T,T)`.  These observations represent
  success or failure of a person trying to run a particular distance
  within a prescribed time on successive days.  It is reasonable to
  assume that this person gets fitter with time.  Analyse this data
  using a logistic regression and justify your use of one-sided or
  two-sided tests.

* Download the `e3mg` dataset from last week.  We are interested in
  the length of an economic recession being greater than five years.
  Create a Bernoulli RV with `1` representing a recession being longer
  than five years and `0` meaning the recession was less than five
  years.  Analyse this using `glm()`.

* Apply `glm()` to a Bernoulli column of your portfolio dataset.  
