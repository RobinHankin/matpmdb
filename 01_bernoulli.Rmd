# The binomial distribution {#binomial}

\newcommand{\prob}[1]{\operatorname{\mathrm{Prob}}\left({#1}\right)}
\newcommand{\like}[1]{\operatorname{\mathcal{L}}\left(#1\right)}
\newcommand{\supp}[1]{\operatorname{\mathcal{S}}\left(#1\right)}


\newcommand{\likeval}{\mathcal{L}}
\newcommand{\suppval}{\mathcal{S}}



## Bernoulli trials

```{block2, type='wikipedia'}
<https://en.wikipedia.org/wiki/Binomial_distribution>
```

A *Bernoulli trial* is an experiment with two possible outcomes.
Examples would include tossing a coin (Heads or Tails) but many other
examples exist.  Observe that any continuous variable may be
transformed into a Bernoulli trial by reporting whether it exceeds a
particular value.  Thus if you are measuring the height of people, the
observation "height is less than 1.8m" is a Bernoulli trial as this
statement may be true or false.

The standard terminology is "success" or "failure" but remember that
these words carry no value judgement.  A Bernoulli trial is completely
characterised by $p$, the probability of success.


## Limiting form of the binomial distribution

```{block2, type='youtube'}
<https://www.youtube.com/watch?v=4q9zDgIm1H4&list=PL018X5Hlr4RkgE65Pg93TFY-32KCVpW84&t=0s&index=2>
```

```{block2, type='youtube'}
<https://www.youtube.com/watch?v=JfYPwu3qWmk&list=PL018X5Hlr4RkgE65Pg93TFY-32KCVpW84&index=2>
```




Given `n` independent Bernoulli trials^[Independence to be defined
formally later in the course; here we say that the drials do not
affect one another.] we are interested in the random variable `r`, the
total number of successes.  We can observe straightaway that $1\leq
r\leq n$ as it is impossible to have more than $n$ successes, or less
than zero.  If the random variable $X$ denotes the number of successes
out of $n$ trials each of probability $p$, it is straightforward to
show that

\begin{equation}
P(X=r) = \frac{n!}{r!(n-r)!}p^r(1-p)^{n-r}
(\#eq:binomialPMF)
\end{equation}


where $n!$ denotes the factorial function (```?factorial```)^[A
question mark is R idiom for accessing the online help system, which
can provide further information.  For example, to get help on the
factorial function, you type ```?factorial``` at the R prompt.]

This is more easily expressed using the "choose notation":

\begin{equation}
{n\choose r} = \frac{n!}{r!(n-r)!}
\end{equation}

Thus

$$P(X=r) = {n\choose r}p^r(1-p)^{n-r}$$


The R programming language has many builtin functions to deal with the
binomial distribution.

We will start with the ```rbinom()``` function^[Remember to type
```?rbinom``` at the R prompt to get help on this function] which
samples from the binomial distribution.  From the help page, this
function takes three arguments: n, size, prob.

Examples:

```{r}
rbinom(100,5,0.5)
```

*You can do this too! The only way to learn is to execute these
 commands yourself*.

In the above, we have 100 samples of size 5, each with a probability
of success of 0.5.  It is as though I give a fair coin to each of 100
students and tell each one to toss the coin 5 times and record the
number of heads (successes).

Observe that if I type the same commands a second time, I get different results:


```{r}
rbinom(100,5,0.5)
```


Now we can change some of the numbers.  Let's vary each one in turn.
First, change the number of observations from 100 to 50:


```{r}
rbinom(50,5,0.5)
```

(we have only 50 results now).  Now change the number of coin tosses from 5 to 20:

```{r}
rbinom(50,20,0.5)
```

(see how the numbers have increased).  Now change the probability of success from 0.5 to 0.9:


```{r}
rbinom(50,20,0.9)
```

(see how the number of successes is now higher, as 90\% of the coin
tosses land heads).  Now change the probability to 0.01:

```{r}
rbinom(50,20,0.01)
```

(most students get zero heads and 20 tails, a few get 1 or 2 heads).

### Summary statistics


Consider again random sampling from the binomial distribution but this
time we would like to summarise a large number of observations, say
1000.  We can use the ```table()``` function:


```{r}
set.seed(0)
table(rbinom(1000,10,0.5))
```

See how easy this is to understand.  There are 7 observations with 1
success, 43 with 2, and so on up to 2 observations with 10 successes.
We can even plot the output:


```{r binomialtable, fig.cap='Table of random binomial observations', out.width='80%', fig.asp=.75, fig.align='center'}
plot(table(rbinom(1000,10,0.5)))
```

(here we simply wrap the ```table()``` function inside a ```plot()```
function).  Figure \@ref(fig:binomialtable) is a powerful
visualisation method.

We can use the binomial probability distribution, equation
\@ref(eq:binomialPMF), in an R session using the ```dbinom()```
function.  Suppose we have a binomial distribution with size 12 and
probability 1/3, and we want to calculate the probability of observing
4 successes.

We can either the mathematical formula:


```{r}
n <- 12
p <- 1/3
r <- 4
factorial(n)/(factorial(r)*factorial(n-r))*p^r*(1-p)^(n-r)
```

but in this case it is much better to use the built-in functionality
```dbinom()```:

```{r}
dbinom(4,12,1/3)
```

See how the two answers above agree with one another.  Note that it is
always better to use the builtin functionality in R, rather than to
use a written function (the builtin version is faster and more
accurate).

One further advantage of using the builtin is that it is vectorized:

```{r}
dbinom(0:12,12,1/3)
```

This shows the probability of observing 0,1,2,...,12 successes out of
12 trials.  It is easy to visualise this using a plot:


```{r}
plot(0:12, dbinom(0:12,12,1/3),type='h')
```


### Numerical verification

It is important when learning new theoretical formulas to verify that
they are correct.  We can do this easily with R, as it includes a
large number of numerical sampling routines.


Consider, for example, the binomial distribution with size 9 and
probability 0.4.  We seek the probability that exactly 3 successes are
observed.  First, calculate the theoretical value:

```{r}
dbinom(3,9,0.4)
```

Now, we will take a random sample and *count* how many are equal to 3:

```{r}
set.seed(0)
table(rbinom(1e6,9,0.4) == 3)
```

(here we take a sample of size one million (```1e6```); the ```==```
is R idiom for asking the question "is the left hand side equal to the
right hand side?", the answer to which is either ```TRUE``` or
```FALSE```).  In this case, The ```TRUE``` count of 250218 shows how
many observations from our random sample were indeed equal to 3.

The probability of a random observation drawn from this binomial
distributionbeing equal to three is thus

```{r}
250218/1e6
```

Note that this closely matches the theoretical value of 0.2508227
given above.  It is possible to vectorize this reasoning and
streamline the idiom.  Say we have size 7 and probability 0.33:

```{r}
dbinom(0:7,7,0.33)
table(rbinom(1e6,7,0.33))/1e6
```

And visualization is possible:

```{r}
theoretical <- dbinom(0:7,7,0.33)
numerical <- table(rbinom(1e6,7,0.33))/1e6
par(pty='s')
plot(theoretical,numerical,asp=1,axes=FALSE)
axis(side=1,at=seq(from=0,to=0.3,by=0.05))
axis(side=2,at=seq(from=0,to=0.3,by=0.05))
abline(0,1)
```

In the plot above, we plot theoretical value on the horizontal axis
and numerical values on the vertical axis.  Exact agreement would mean
the points are exactly on the diagonal line; see how close the
agreement is.
